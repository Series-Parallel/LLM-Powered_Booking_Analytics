# -*- coding: utf-8 -*-
"""Hotel_Booking_Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iiSEECTPiX1ryH52bfRIOmgFA4Z7-2f7
"""

import pandas as pd

df = pd.read_csv("hotel_bookings.csv")

df.head()

df.info()

df['company'].unique()

df['company'] = df['company'].fillna(0)

df['company'].unique()

df.info()

df['agent'].unique()

df['agent'] = df['agent'].fillna(0)

df['children'].unique()

df['children'] = df['children'].fillna(0)

df.info()

df['country'].unique()

df['country']

df['country'] = df['country'].fillna('Unknown')

df.isna().sum()

"""## Analysis!"""

import seaborn as sns
import matplotlib.pyplot as plt

"""### 1. Revenue over time

the only feature realted to price is the adr column which is defined as: Average Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights. So in order to find revenue we will do

 * Revenue = adr * (total nights)

And then we will plot it against time as we are already provided with year, month and date columns.

"""

df['Revenue'] = df['adr'] * (df['stays_in_week_nights'] + df['stays_in_weekend_nights'])

df['arrival_date'] = pd.to_datetime(df['arrival_date_year'].astype(str) + '-' +
                                    df['arrival_date_month'] + '-' +
                                    df['arrival_date_day_of_month'].astype(str),
                                    format='%Y-%B-%d')

df.info()

"""we are converting dates into yy/mm/dd format because pandas is compantible with that format!"""

df['arrival_date'].head()

revenue_trends = df.groupby(df['arrival_date'].dt.to_period('M'))['Revenue'].sum()

plt.figure(figsize=(10, 6))
revenue_trends.plot(kind='line',marker='o', color='b')
plt.title('Revenue Trends Over Time')
plt.xlabel('Month')
plt.ylabel('Revenue')
plt.xticks(rotation=45)
plt.show()

"""### 2. Cancellation rate as percentage of total bookings


**From Google** *I found that cancellation rate is equal to the number of bookings cancelled divied by the total number of bookings*

* Cancellation rate = ({# is_cancelled=1})/(total # of bookings)
"""

cancelled_booking = df[df['is_canceled'] == 1]['is_canceled'].sum()
cancelled_booking

total_bookings = df['is_canceled'].count()
total_bookings

Cancelation_Rate = (cancelled_booking/total_bookings)*100
print(f"The cancellation rate is {Cancelation_Rate}%")

plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='is_canceled', hue='is_canceled', bins=2, palette=['orange', 'blue'])
plt.title('Cancellation vs Not Canceled')
plt.xlabel('Booking Status (0: Not Cancelled, 1: Cancelled)')
plt.ylabel('Count')
plt.show()

"""## 3. Geographical distribution of users doing the bookings

we will make distribution of countries that are in the bookings
"""

df['country'].unique()

country_counts = df["country"].value_counts()

top_countries = country_counts.head(20)

plt.figure(figsize=(12, 6))
sns.barplot(x=top_countries.index, y=top_countries.values, palette="viridis")
plt.xticks(rotation=45)
plt.xlabel("Country")
plt.ylabel("Number of Bookings")
plt.title("Top 20 Countries by Number of Bookings")
plt.show()

"""## 4. Booking Lead time Distribution"""

plt.figure(figsize=(10, 6))
sns.histplot(df['lead_time'], bins=30, kde=True)
plt.title('Booking Lead Time Distribution')
plt.xlabel('Lead Time (Days)')
plt.ylabel('Number of Bookings')
plt.show()

"""## 5. Types of Hotels"""

hotel_counts = df['hotel'].value_counts()

plt.figure(figsize=(8, 6))
sns.barplot(x=hotel_counts.index, y=hotel_counts.values)
plt.title('Types of Hotels')
plt.xlabel('Hotel Type')
plt.ylabel('Number of Bookings')
plt.show()

"""# RAG MODEL"""

import torch
print("GPU Available:", torch.cuda.is_available())
print("GPU Name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")

"""### installations!"""

!pip install chromadb

!pip install langchain_community

"""## creating vector databse using ChromaDB

"""

from langchain_community.embeddings import HuggingFaceEmbeddings
import chromadb

embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

df.info()

def create_text_representation(row):
    return f"""
    Hotel Type: {row['hotel']}
    Country: {row['country']}
    Market Segment: {row['market_segment']}
    Arrival Date: {row['arrival_date']}
    Number of Adults: {row['adults']}
    Number of Children: {row['children']}
    Number of Babies: {row['babies']}
    Meal Plan: {row['meal']}
    Market Segment: {row['market_segment']}
    Distribution Segment: {row['distribution_channel']}
    Number of days in waiting list: {row['days_in_waiting_list']} days
    Deposit Type: {row['deposit_type']}
    Customer Type: {row['customer_type']}
    Lead Time: {row['lead_time']} days
    Stay Duration: {row['stays_in_weekend_nights']} weekend nights, {row['stays_in_week_nights']} week nights
    Booking Canceled: {row['is_canceled']}
    Average Daily Rate: ${row['adr']}
    Revenue: ${row['Revenue']}
    Reservation Status: {row['reservation_status']}
    Reservation Status Date: {row['reservation_status_date']}
    """

df["text_representation"] = df.apply(create_text_representation, axis=1)

df["text_representation"][0]

chroma_client = chromadb.PersistentClient(path="./chroma_db")

collection = chroma_client.get_or_create_collection(name="hotel_bookings")

import numpy as np

analytics = {
    "Total revenue of the document": df["Revenue"].sum(),
    "average adr of the document": df["adr"].mean(),
    "cancellation rate": df["is_canceled"].mean(),
    "most popular hotel": df["hotel"].mode()[0],
    "most popular market segment": df["market_segment"].mode()[0],
    "most popular distribution channel": df["distribution_channel"].mode()[0],
    "most popular customer type": df["customer_type"].mode()[0],
    "most popular meal plan": df["meal"].mode()[0],
}

"""now we are adding the analytics to the vector database so it will be easy to fetch responses to the queries related to the analytics!"""

analytics_texts = list(analytics.values())
analytics_texts

analytics_embedding = embeddings_model.embed_query(str(analytics_texts))

collection.add(
    ids=["analytics_summary"],
    metadatas=[analytics],
    documents=["Precomputed analytics summary"],
    embeddings= analytics_embedding
)
print("✅ Precomputed analytics stored successfully!")

batch_size = 1000
num_batches = int(np.ceil(len(df) / batch_size))

for i in range(num_batches):
    batch = df.iloc[i * batch_size:(i + 1) * batch_size]

    texts = batch["text_representation"].tolist()


    embeddings = embeddings_model.embed_documents(texts)

    ids = [str(index) for index in batch.index]
    metadatas = [
        {
            "text_representation": row["text_representation"],
            "hotel": row["hotel"],
            "country": row["country"],
            "market_segment": row["market_segment"],
            "customer_type": row["customer_type"],
            "lead_time": row["lead_time"],
            "stay_duration": f"{row['stays_in_weekend_nights']} weekend nights, {row['stays_in_week_nights']} week nights",
            "is_canceled": row["is_canceled"],
            "adr": row["adr"],
            "special_requests": row["total_of_special_requests"],
            "total_revenue": row["Revenue"],
            "reservation_status": row["reservation_status"],
            "arrival_date": str(row["arrival_date"])
        }
        for _, row in batch.iterrows()
    ]

    collection.add(
        ids=ids,
        embeddings=embeddings,
        documents=texts,
        metadatas=metadatas
    )
    print(f"✅ Batch {i+1}/{num_batches} added successfully!")

"""### now will make llm with RAG"""

from transformers import AutoModelForCausalLM, AutoTokenizer

llm_name = "mistralai/Mistral-7B-v0.1"
tokenizer = AutoTokenizer.from_pretrained(llm_name)
llm = AutoModelForCausalLM.from_pretrained(llm_name, torch_dtype=torch.float16, device_map="auto")

import torch

def query_rag(user_query, top_k=3):
    # Get query embedding
    query_embedding = embeddings_model.embed_query(user_query)

    # Retrieve top-k similar documents
    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)

    retrieved_docs = [doc for doc in results["documents"][0]]

    retrieved_metadata = [
        f"Total Revenue: {meta.get('total_revenue', 'N/A')}, ADR: {meta.get('adr', 'N/A')}, Stay Duration: {meta.get('stay_duration', 'N/A')}"
        for meta in results["metadatas"][0]
    ]


    analytics_doc = collection.get(ids=["analytics_summary"])
    analytics_metadata = str(analytics_doc.get("metadatas", [{}])[0])  # Safe extraction


    retrieved_docs.extend(retrieved_metadata)
    retrieved_docs.append(analytics_metadata)

    context = "\n\n".join([doc.strip() for doc in retrieved_docs if doc.strip()])

    prompt = f"""
You are an AI assistant tasked with answering the given question based on the retrieved documents.
Only use the provided information to generate an accurate and concise response.
Do not make up information or speculate. If the retrieved documents do not contain relevant information, state that explicitly.
Only answer the question given in the query and do not include any other questions or answers.

Context:
{context}


Query Question: {user_query}
Answer:
"""

    # Ensure proper device selection
    device = "cuda" if torch.cuda.is_available() else "cpu"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    output = llm.generate(**inputs, max_new_tokens=200)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)

    return answer

response = query_rag("What is the total revenue of the document?")
print("LLM Response:", response)

print("Total vectors in collection:", collection.count())

print("DEBUG: Sample Document:", collection.query(query_embeddings=[[0]*384], n_results=1))

analytics_doc = collection.get(ids=["analytics_summary"])
analytics_doc

"""# Flask API Integration"""

!pip install flask flask_cors pyngrok

import os

from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok

from google.colab import userdata

app = Flask(__name__)
CORS(app)

@app.route("/", methods=["POST"])
def generate():
    data = request.json
    user_query = data.get("query", "")

    if not user_query:
        return jsonify({"error": "No query provided"}), 400

    prompt = f"Answer the question based on the given context:\n{user_query}"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = llm.generate(**inputs, max_new_tokens=200)
    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return jsonify({"response": response_text})

auth_token = userdata.get('NGROK_TOKEN')


ngrok.set_auth_token(auth_token)
public_url = ngrok.connect(5000)
print(f"Public URL: {public_url}")


app.run(port=5000)

"""#### Downloading chroma db"""

!zip -r chroma_db.zip /content/chroma_db

from google.colab import files
files.download("chroma_db.zip")